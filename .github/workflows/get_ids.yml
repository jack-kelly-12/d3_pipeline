name: Get NCAA IDs

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch:

concurrency:
  group: ncaa-scraper
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 300

    # Explicitly request write permissions
    permissions:
      contents: write

    env:
      DATA_DIR: data

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          # Using GITHUB_TOKEN for checkout
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests beautifulsoup4 tqdm

      - name: Run scraper
        id: scraper
        timeout-minutes: 1
        continue-on-error: true
        run: |
          python scrapers/get_all_ids.py --data_dir ${{ env.DATA_DIR }}

      - name: Commit and push progress
        if: always()
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Stage data directory and any output files
          git add data/ scraper.log scraper_progress.bak || true

          # Check if there are changes to commit
          if [[ -n "$(git status --porcelain)" ]]; then
            # Commit the changes
            git commit -m "Updated ids"
            
            # Fetch latest from remote
            git fetch origin
            
            # Try to rebase our changes on top of the latest remote
            git rebase origin/${GITHUB_REF#refs/heads/} || {
              # If rebase fails, reset to the last commit and try again
              echo "Rebase failed, trying alternative approach..."
              git rebase --abort
              
              # Reset to the latest remote
              git reset --hard origin/${GITHUB_REF#refs/heads/}
              
              # Re-add our changes
              git add data/ scraper.log scraper_progress.bak || true
              
              # Check if we still have changes after reset
              if [[ -n "$(git status --porcelain)" ]]; then
                git commit -m "Updated ids"
              else
                echo "No changes after reset, skipping commit"
                exit 0
              fi
            }
            
            # Push changes, with a slight delay to avoid race conditions
            sleep 2
            git push "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"
            echo "Changes committed and pushed successfully"
          else
            echo "No changes to commit"
          fi

      # Final cleanup step
      - name: Cleanup
        if: always()
        run: |
          # Add any cleanup tasks here
          echo "Scraper completed with status: ${{ steps.scraper.outcome }}"
